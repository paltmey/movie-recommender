{
 "cells": [
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf.config.optimizer.set_jit(True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Detect hardware\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
    "except ValueError:\n",
    "  tpu = None\n",
    "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "    \n",
    "# Select appropriate distribution strategy for hardware\n",
    "if tpu:\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "  print('Running on TPU ', tpu.master())  \n",
    "elif len(gpus) > 0:\n",
    "  strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs\n",
    "  print('Running on ', len(gpus), ' GPU(s) ')\n",
    "else:\n",
    "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "  print('Running on CPU')\n",
    "\n",
    "# How many accelerators do we have ?\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Bidirectional, GRU, LSTM\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# WandB â€“ Install the W&B library\n",
    "%pip install wandb --upgrade\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_ENTITY'] = \"\"\n",
    "os.environ['WANDB_PROJECT'] = \"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10000\n",
    "MAX_SEQ_LENGTH = 15\n",
    "NUM_CLASSES = 500 + 1 # vocab size + 1 for masking\n",
    "SHUFFLE_BUFFER_SIZE = 10000"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "config_defaults = {\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate': 0.005,\n",
    "    'dropout': 0.0,\n",
    "    'embedding_dims': 512,\n",
    "    'rnn_units': 128,\n",
    "    'rnn_type': 'gru',\n",
    "    'bidirectional': 0,\n",
    "    'stack_size': 1\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "LOG_DIR = 'runs'\n",
    "TRAINING_DATASET_PATTERN = '../*_train_*.tfrec'\n",
    "VALIDATION_DATASET_PATTERN = '../*_test_*.tfrec'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    regex = re.compile(r'_([0-9]*)\\.')\n    n = [int(regex.search(filename).group(1)) for filename in filenames]\n    return sum(n)\n\ndef read_tfrecord(example):\n    features = {\n        'rating_chunk': tf.io.VarLenFeature(tf.int64),\n        'label': tf.io.FixedLenFeature([], tf.int64)\n    }\n\n    example = tf.io.parse_single_example(example, features)\n\n    rating_chunk = tf.sparse.to_dense(example['rating_chunk'])\n    paddings = [[0, 0], [0, MAX_SEQ_LENGTH - len(rating_chunk)]]\n\n    rating_chunk = tf.reshape(rating_chunk, (1, -1))\n    rating_chunk = tf.pad(rating_chunk, paddings, 'CONSTANT')\n    rating_chunk = rating_chunk[0]\n\n    label = tf.one_hot(example['label'] + 1, NUM_CLASSES)  # TODO num classes to arg\n    return rating_chunk, label\n\ndef load_dataset(filenames):\n    # read from TFRecords. For optimal performance, read from multiple\n    # TFRecord files at once and set the option experimental_deterministic = False\n    # to allow order-altering optimizations.\n\n    option_no_order = tf.data.Options()\n    option_no_order.experimental_deterministic = False\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.with_options(option_no_order)\n\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n    return dataset\n\ndef get_training_dataset(filenames, batch_size, shuffle_buffer_size):\n    dataset = load_dataset(filenames)\n    dataset = dataset.cache()\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(filenames, batch_size):\n    dataset = load_dataset(filenames)\n    dataset = dataset.cache()\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    \n    # needed for TPU 32-core pod: the test dataset has only 3 files but there are 4 TPUs. FILE sharding policy must be disabled.\n    opt = tf.data.Options()\n    opt.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n    dataset = dataset.with_options(opt)\n    \n    return dataset",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def create_model(num_classes, optimizer_name, learning_rate, dropout, embedding_dims, rnn_units, rnn_type, bidirectional, stack_size):    \n    model = Sequential()\n    model.add(Embedding(num_classes, embedding_dims, mask_zero=True, input_length=MAX_SEQ_LENGTH)) # TODO input_length needed?\n        \n    def add_rnn_layer(model, rnn_type, bidirectinonal, dropout, return_sequences):\n        if rnn_type == 'lstm':\n            if bidirectional:\n                model.add(Bidirectional(LSTM(rnn_units, return_sequences=return_sequences)))\n            else:\n                model.add(LSTM(rnn_units, return_sequences=return_sequences))\n        else:\n            if bidirectional:\n                model.add(Bidirectional(GRU(rnn_units, return_sequences=return_sequences)))\n            else:\n                model.add(GRU(rnn_units, return_sequences=return_sequences))\n        model.add(Dropout(dropout))\n\n    for i in range(1, stack_size):\n        add_rnn_layer(model, rnn_type, bidirectional, dropout, True)\n    \n    add_rnn_layer(model, rnn_type, bidirectional, dropout, False)\n    \n    model.add(Dense(num_classes))\n    model.add(Activation('softmax', dtype='float32'))\n    \n    if optimizer_name == 'adam':\n        optimizer = Adam(learning_rate=learning_rate)\n    else:\n        optimizer = RMSprop(learning_rate=learning_rate)\n    \n    metrics = [keras.metrics.CategoricalAccuracy(),\n               keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_categorical_accuracy'),\n               keras.metrics.TopKCategoricalAccuracy(k=10, name='top_10_categorical_accuracy'),\n               keras.metrics.TopKCategoricalAccuracy(k=MAX_SEQ_LENGTH, name=f'top_{MAX_SEQ_LENGTH}_categorical_accuracy')]\n    \n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=metrics)\n    \n    return model",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def train(config):\n    print('Start training')\n    # Initilize a new wandb run\n    wandb.init(config=config)\n    \n    print('Reading dataset')\n    training_filenames = tf.io.gfile.glob(TRAINING_DATASET_PATTERN)\n    validation_filenames = tf.io.gfile.glob(VALIDATION_DATASET_PATTERN)\n\n    train_steps = count_data_items(training_filenames) // BATCH_SIZE\n\n    training_data = get_training_dataset(training_filenames, BATCH_SIZE, SHUFFLE_BUFFER_SIZE)\n    validation_data = get_validation_dataset(validation_filenames, BATCH_SIZE)\n    \n    print('Creating model')    \n    model = create_model(NUM_CLASSES,\n                         config['optimizer'],\n                         config['learning_rate'],\n                         config['dropout'],\n                         config['embedding_dims'],\n                         config['rnn_units'],\n                         config['rnn_type'],\n                         config['bidirectional'],\n                         config['stack_size'])\n    model.summary()\n    \n    #model_checkpoint = keras.callbacks.ModelCheckpoint('model.hdf5', monitor='val_categorical_accuracy', save_best_only=True, period=5)\n    early_stopping = keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=5, mode='auto')\n    wandb_callback = WandbCallback(monitor='val_categorical_accuracy')\n\n    callbacks = [wandb_callback, early_stopping]\n    \n    print('Start training')\n    model.fit(training_data, validation_data=validation_data, epochs=EPOCHS, steps_per_epoch=train_steps, callbacks=callbacks)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "param_grid = [{'rnn_units': 512, 'embedding_dims':128, 'dropout':0.5, 'bidirectional':1, 'stack_size':2}]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "for param_config in param_grid:\n    config = config_defaults.copy()\n    config.update(param_config)\n    train(config)",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}